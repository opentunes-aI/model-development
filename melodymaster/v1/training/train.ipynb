{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽµ MelodyMaster V1 Training\n",
    "\n",
    "Fine-tuning MusicGen Melody model on MusicCaps dataset. This notebook works in both local Jupyter and Google Colab environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Install dependencies in Colab\n",
    "    !pip install transformers datasets torch accelerate pyyaml -q\n",
    "    \n",
    "    # Optional: Mount Google Drive if needed\n",
    "    from google.colab import drive\n",
    "    # Uncomment to mount drive\n",
    "    # drive.mount('/content/drive')\n",
    "    \n",
    "    # Check GPU\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    # Local dependencies should be installed in the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import yaml\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Enter your token in the next cell and run this\n",
    "def setup_hf_auth(token):\n",
    "    login(token=token)\n",
    "    print(\"Successfully logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Enter your HF token here\n",
    "HF_TOKEN = \"your_token_here\"  # Replace with your token\n",
    "setup_hf_auth(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load config based on environment\n",
    "def load_config():\n",
    "    if IN_COLAB:\n",
    "        # If config.yaml is in Drive or needs to be created\n",
    "        config = {\n",
    "            'model': {\n",
    "                'base_model': 'facebook/musicgen-melody',\n",
    "                'output_model': 'opentunesai/melodymaster-v1'\n",
    "            },\n",
    "            'training': {\n",
    "                'batch_size': 4,\n",
    "                'learning_rate': 2e-5,\n",
    "                'num_epochs': 3\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        # Load from local config file\n",
    "        with open('config.yaml', 'r') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "config = load_config()\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config['model']['base_model'],\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(config['model']['base_model'])\n",
    "\n",
    "# Load a small subset of MusicCaps for testing\n",
    "dataset = load_dataset(\"google/musiccaps\", split=\"train[:100]\")\n",
    "print(f\"Loaded {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize accelerator\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['training']['learning_rate']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training implementation here\n",
    "def train():\n",
    "    model.train()\n",
    "    # Training loop implementation\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Push to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save and push model to HF\n",
    "model.push_to_hub(config['model']['output_model'])\n",
    "processor.push_to_hub(config['model']['output_model'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU"
 }
}
